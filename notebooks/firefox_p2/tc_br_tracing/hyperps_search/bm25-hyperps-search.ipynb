{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - Hyperparameters Search\n",
    "\n",
    "In this notebook we demonstrate the use of **BM25 (Best Matching 25)** Information Retrieval technique to make trace link recovery between Test Cases and Bug Reports.\n",
    "\n",
    "We model our study as follows:\n",
    "\n",
    "* Each bug report title, summary and description compose a single query.\n",
    "* We use each test case content as an entire document that must be returned to the query made\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod_finder_util import mod_finder_util\n",
    "mod_finder_util.add_modules_origin_search_path()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "\n",
    "from modules.utils import plots\n",
    "from modules.utils import firefox_dataset_p2 as fd\n",
    "from modules.utils import tokenizers as tok\n",
    "from modules.utils import aux_functions\n",
    "from modules.utils import model_evaluator as m_eval\n",
    "\n",
    "from modules.models.bm25 import BM_25\n",
    "from modules.models.model_hyperps import BM25_Model_Hyperp\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestCases.shape: (207, 12)\n",
      "SelectedBugReports2.shape: (93, 22)\n",
      "OracleExpertVolunteers.shape: (207, 93)\n"
     ]
    }
   ],
   "source": [
    "test_cases_df = fd.Datasets.read_testcases_df()\n",
    "bug_reports_df = fd.Datasets.read_selected_bug_reports_2_df()\n",
    "\n",
    "corpus = test_cases_df.tc_desc\n",
    "query = bug_reports_df.br_desc\n",
    "\n",
    "test_cases_names = test_cases_df.tc_name\n",
    "bug_reports_names = bug_reports_df.br_name\n",
    "\n",
    "orc = fd.Tc_BR_Oracles.read_oracle_expert_volunteers_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestCases Subset Shape: (14, 10)\n",
      "BugReports Subset Shape: (15, 12)\n",
      "Oracle Subset Shape: (14, 15)\n"
     ]
    }
   ],
   "source": [
    "bugreports_subset_df = bug_reports_df[(bug_reports_df.Version == '48 Branch') | (bug_reports_df.Version == '60 Branch')].sample(15, random_state=42)\n",
    "testcases_subset_df = test_cases_df[(test_cases_df.TestDay.str.contains('20161014')) | (test_cases_df.TestDay.str.contains('20161028'))].sample(10, random_state=1000)\n",
    "\n",
    "selected_testcases = ['TC_{}_TRG'.format(tc_num) for tc_num in [13, 14, 15, 16, 17, 18]]  # should link with 48 Branch\n",
    "aux_tc = test_cases_df[test_cases_df.tc_name.isin(selected_testcases)]\n",
    "\n",
    "tc_subset_df = testcases_subset_df.append(aux_tc)\n",
    "tc_subset_df.drop_duplicates(inplace=True)\n",
    "\n",
    "corpus_subset = tc_subset_df.tc_desc\n",
    "query_subset = bugreports_subset_df.br_desc\n",
    "testcases_names_subset = tc_subset_df.tc_name\n",
    "bug_reports_names_subset = bugreports_subset_df.br_name\n",
    "orc_subset_df = orc.loc[testcases_names_subset, bug_reports_names_subset]\n",
    "\n",
    "print('TestCases Subset Shape: {}'.format(tc_subset_df.shape))\n",
    "print('BugReports Subset Shape: {}'.format(bugreports_subset_df.shape))\n",
    "print('Oracle Subset Shape: {}'.format(orc_subset_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find The Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model hyperparameters search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:    0.2s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "all_hyperparams = {\n",
    "    BM25_Model_Hyperp.TOP.value : [3,5],\n",
    "    BM25_Model_Hyperp.SIM_MEASURE_MIN_THRESHOLD.value : [('-', 0.0)],\n",
    "    BM25_Model_Hyperp.TOKENIZER.value : [tok.PorterStemmerBased_Tokenizer(), tok.LancasterStemmerBased_Tokenizer(), \n",
    "                                         tok.WordNetBased_LemmaTokenizer(), tok.SnowballStemmerBased_Tokenizer()]\n",
    "}\n",
    "\n",
    "hyperparams = aux_functions.generate_params_comb_list(**all_hyperparams)\n",
    "\n",
    "print('Performing model hyperparameters search...')\n",
    "\n",
    "def run_model(idx, **hyperp):    \n",
    "    current_model = BM_25(**hyperp)\n",
    "    current_model.set_name('BM25_Model_{}'.format(idx))\n",
    "    current_model.recover_links(corpus_subset, query_subset, testcases_names_subset, bug_reports_names_subset)\n",
    "    \n",
    "    evaluator = m_eval.ModelEvaluator(orc_subset_df, current_model)\n",
    "    evaluator.evaluate_model()\n",
    "    evaluator.dump_model()\n",
    "    evaluator.dump_evaluator()\n",
    "    \n",
    "    return([evaluator.get_mean_precision(), \n",
    "            evaluator.get_mean_recall(),\n",
    "            evaluator.get_mean_fscore(), \n",
    "            evaluator.get_model().get_name(),\n",
    "            evaluator.get_model().get_top_value(),\n",
    "            evaluator.get_model().get_tokenizer_type(),\n",
    "            evaluator.get_model().get_model_dump_path(),\n",
    "            evaluator.get_evaluator_dump_path()\n",
    "           ])\n",
    "\n",
    "tasks = [(idx,hp) for idx,hp in enumerate(hyperparams)]\n",
    "results = Parallel(n_jobs=-1, verbose=1)(delayed(run_model)(idx,**hp) for idx,hp in tasks)\n",
    "results_df = pd.DataFrame(data=results, \n",
    "                          columns=['precision', 'recall', 'fscore', 'model_name', 'top_value', 'tokenizer', 'model_dump', 'evaluator_dump'])\n",
    "results_df = results_df.astype(dtype={'model_dump' : str, 'evaluator_dump' : str})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Report -------------------\n",
      "\n",
      "Total of Analyzed Hyperparameters Combinations: 8\n",
      "\n",
      "Best Model Hyperparameters Combination Found:\n",
      "\n",
      "{'Measures': {'Mean FScore of BM25_Model_6': 0.13333333333333333,\n",
      "              'Mean Precision of BM25_Model_6': 0.16,\n",
      "              'Mean Recall of BM25_Model_6': 0.11428571428571428},\n",
      " 'Setup': [{'Name': 'BM25_Model_6'},\n",
      "           {'Top Value': 5},\n",
      "           {'Sim Measure Min Threshold': ('-', 0.0)},\n",
      "           {'K': 1.2},\n",
      "           {'B': 0.75},\n",
      "           {'Epsilon': 0.25},\n",
      "           {'Tokenizer Type': <class 'utils.tokenizers.WordNetBased_LemmaTokenizer'>}]}\n"
     ]
    }
   ],
   "source": [
    "best_model = aux_functions.report_best_model(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save_sim_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model for TOP 3 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Measures': {'Mean FScore of BM25_Model_2': 0.10666666666666667,\n",
      "              'Mean Precision of BM25_Model_2': 0.17777777777777776,\n",
      "              'Mean Recall of BM25_Model_2': 0.07619047619047618},\n",
      " 'Setup': [{'Name': 'BM25_Model_2'},\n",
      "           {'Top Value': 3},\n",
      "           {'Sim Measure Min Threshold': ('-', 0.0)},\n",
      "           {'K': 1.2},\n",
      "           {'B': 0.75},\n",
      "           {'Epsilon': 0.25},\n",
      "           {'Tokenizer Type': <class 'utils.tokenizers.WordNetBased_LemmaTokenizer'>}]}\n",
      "------------------------------------------------------------------\n",
      "{'Measures': {'Mean FScore of BM25_Model_6': 0.13333333333333333,\n",
      "              'Mean Precision of BM25_Model_6': 0.16,\n",
      "              'Mean Recall of BM25_Model_6': 0.11428571428571428},\n",
      " 'Setup': [{'Name': 'BM25_Model_6'},\n",
      "           {'Top Value': 5},\n",
      "           {'Sim Measure Min Threshold': ('-', 0.0)},\n",
      "           {'K': 1.2},\n",
      "           {'B': 0.75},\n",
      "           {'Epsilon': 0.25},\n",
      "           {'Tokenizer Type': <class 'utils.tokenizers.WordNetBased_LemmaTokenizer'>}]}\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "aux_functions.print_report_top_3_and_5_v1(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - Hyperparameters Search\n",
    "\n",
    "In this notebook we demonstrate the use of **Word Embeddings (Word2Vec)** weighting technique into Information Retrieval to make trace link recovery between Test Cases and Bug Reports.\n",
    "\n",
    "We model our study as follows:\n",
    "\n",
    "* Each bug report title, summary and description compose a single query.\n",
    "* We use each test case content as an entire document that must be returned to the query made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod_finder_util import mod_finder_util\n",
    "mod_finder_util.add_modules_origin_search_path()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "\n",
    "from modules.utils import plots\n",
    "from modules.utils import firefox_dataset_p2 as fd\n",
    "from modules.utils import tokenizers as tok\n",
    "from modules.utils import aux_functions\n",
    "from modules.utils import model_evaluator as m_eval\n",
    "\n",
    "from modules.models.wordvec import SimilarityMeasure\n",
    "from modules.models.wordvec import WordVec_BasedModel\n",
    "from modules.models.model_hyperps import WordVec_Model_Hyperp\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestCases.shape: (207, 12)\n",
      "BugReports.shape: (93, 19)\n",
      "Oracle.shape: (207, 93)\n"
     ]
    }
   ],
   "source": [
    "test_cases_df = fd.Datasets.read_testcases_df()\n",
    "bug_reports_df = fd.Datasets.read_selected_bug_reports_2_df()\n",
    "\n",
    "corpus = test_cases_df.tc_desc\n",
    "query = bug_reports_df.br_desc\n",
    "\n",
    "test_cases_names = test_cases_df.tc_name\n",
    "bug_reports_names = bug_reports_df.br_name\n",
    "\n",
    "orc = fd.Tc_BR_Oracles.read_oracle_expert_volunteers_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestCases Subset Shape: (14, 10)\n",
      "BugReports Subset Shape: (15, 12)\n",
      "Oracle Subset Shape: (14, 15)\n"
     ]
    }
   ],
   "source": [
    "bugreports_subset_df = bug_reports_df[(bug_reports_df.Version == '48 Branch') | (bug_reports_df.Version == '60 Branch')].sample(15, random_state=42)\n",
    "testcases_subset_df = test_cases_df[(test_cases_df.TestDay.str.contains('20161014')) | (test_cases_df.TestDay.str.contains('20161028'))].sample(10, random_state=1000)\n",
    "\n",
    "selected_testcases = ['TC_{}_TRG'.format(tc_num) for tc_num in [13, 14, 15, 16, 17, 18]]  # should link with 48 Branch\n",
    "aux_tc = test_cases_df[test_cases_df.tc_name.isin(selected_testcases)]\n",
    "\n",
    "tc_subset_df = testcases_subset_df.append(aux_tc)\n",
    "tc_subset_df.drop_duplicates(inplace=True)\n",
    "\n",
    "corpus_subset = tc_subset_df.tc_desc\n",
    "query_subset = bugreports_subset_df.br_desc\n",
    "testcases_names_subset = tc_subset_df.tc_name\n",
    "bug_reports_names_subset = bugreports_subset_df.br_name\n",
    "orc_subset_df = orc.loc[testcases_names_subset, bug_reports_names_subset]\n",
    "\n",
    "print('TestCases Subset Shape: {}'.format(tc_subset_df.shape))\n",
    "print('BugReports Subset Shape: {}'.format(bugreports_subset_df.shape))\n",
    "print('Oracle Subset Shape: {}'.format(orc_subset_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find The Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model hyperparameters search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:10, 10.51s/it]\u001b[A\n",
      "2it [00:21, 10.52s/it]\u001b[A\n",
      "3it [00:32, 10.90s/it]\u001b[A\n",
      "4it [00:43, 10.81s/it]\u001b[A\n",
      "5it [00:54, 10.77s/it]\u001b[A\n",
      "6it [01:04, 10.72s/it]\u001b[A\n",
      "7it [01:15, 10.65s/it]\u001b[A\n",
      "8it [01:25, 10.65s/it]\u001b[A\n",
      "9it [01:36, 10.66s/it]\u001b[A\n",
      "10it [01:47, 10.64s/it]\u001b[A\n",
      "11it [01:58, 10.72s/it]\u001b[A\n",
      "12it [02:08, 10.76s/it]\u001b[A\n",
      "13it [02:19, 10.75s/it]\u001b[A\n",
      "14it [02:31, 11.04s/it]\u001b[A\n",
      "15it [02:42, 11.20s/it]\u001b[A\n",
      "16it [02:54, 11.35s/it]\u001b[A\n",
      "17it [03:06, 11.41s/it]\u001b[A\n",
      "18it [03:17, 11.42s/it]\u001b[A\n",
      "19it [03:28, 11.35s/it]\u001b[A\n",
      "20it [03:40, 11.32s/it]\u001b[A\n",
      "21it [03:50, 11.17s/it]\u001b[A\n",
      "22it [04:02, 11.22s/it]\u001b[A\n",
      "23it [04:13, 11.22s/it]\u001b[A\n",
      "24it [04:24, 11.11s/it]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "all_hyperparams = {\n",
    "    WordVec_Model_Hyperp.SIM_MEASURE_MIN_THRESHOLD.value : [('cosine' ,x)  for x in [.75,.85,.95]],\n",
    "    WordVec_Model_Hyperp.TOP.value : [3,5],\n",
    "    WordVec_Model_Hyperp.TOKENIZER.value : [tok.PorterStemmerBased_Tokenizer(), tok.LancasterStemmerBased_Tokenizer(), \n",
    "                                                   tok.WordNetBased_LemmaTokenizer(), tok.SnowballStemmerBased_Tokenizer()]\n",
    "}\n",
    "\n",
    "hyperparams = aux_functions.generate_params_comb_list(**all_hyperparams)          \n",
    "\n",
    "print('Performing model hyperparameters search...')\n",
    "\n",
    "results_df = pd.DataFrame(columns=['precision', 'recall', 'fscore', 'model_name', 'top_value', 'tokenizer', 'metric', 'metric_value', 'model_dump', 'evaluator_dump'])\n",
    "\n",
    "#def run_model(idx, **hyperp):    \n",
    "for idx,hp in tqdm(enumerate(hyperparams)):\n",
    "    current_model = WordVec_BasedModel(**hp)\n",
    "    current_model.set_name('WordVec_Based_Model_{}'.format(idx))\n",
    "    current_model.recover_links(corpus_subset, query_subset, testcases_names_subset, bug_reports_names_subset)\n",
    "    \n",
    "    evaluator = m_eval.ModelEvaluator(orc_subset_df, current_model)\n",
    "    evaluator.evaluate_model()\n",
    "    evaluator.dump_model()\n",
    "    evaluator.dump_evaluator()\n",
    "    \n",
    "    results_df = results_df.append(pd.DataFrame([[evaluator.get_mean_precision(), \n",
    "                    evaluator.get_mean_recall(),\n",
    "                    evaluator.get_mean_fscore(), \n",
    "                    evaluator.get_model().get_name(),\n",
    "                    evaluator.get_model().get_top_value(),\n",
    "                    evaluator.get_model().get_tokenizer_type(),\n",
    "                    evaluator.get_model().get_sim_measure_min_threshold()[0],\n",
    "                    evaluator.get_model().get_sim_measure_min_threshold()[1],\n",
    "                    evaluator.get_model().get_model_dump_path(),\n",
    "                    evaluator.get_evaluator_dump_path()\n",
    "           ]], columns=results_df.columns), ignore_index=True)\n",
    "\n",
    "#tasks = [(idx,hp) for idx,hp in enumerate(hyperparams)]\n",
    "#results = Parallel(n_jobs=-1, verbose=1)(delayed(run_model)(idx, **hp) for idx,hp in tasks)\n",
    "results_df = results_df.astype(dtype={'model_dump' : str, 'evaluator_dump' : str, 'top_value': int})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Report -------------------\n",
      "\n",
      "Total of Analyzed Hyperparameters Combinations: 24\n",
      "\n",
      "Best Model Hyperparameters Combination Found:\n",
      "\n",
      "{'Measures': {'Mean FScore of WordVec_Based_Model_6': 0.16666666666666663,\n",
      "              'Mean Precision of WordVec_Based_Model_6': 0.2,\n",
      "              'Mean Recall of WordVec_Based_Model_6': 0.14285714285714285},\n",
      " 'Setup': [{'Name': 'WordVec_Based_Model_6'},\n",
      "           {'Similarity Measure and Minimum Threshold': ('cosine', 0.75)},\n",
      "           {'Top Value': 5},\n",
      "           {'Tokenizer': <utils.tokenizers.WordNetBased_LemmaTokenizer object at 0x7fb49aa33e80>}]}\n"
     ]
    }
   ],
   "source": [
    "best_model = aux_functions.report_best_model(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save_sim_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model for TOP 3 and 5 - Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Measures': {'Mean FScore of WordVec_Based_Model_2': 0.14666666666666667,\n",
      "              'Mean Precision of WordVec_Based_Model_2': 0.24444444444444444,\n",
      "              'Mean Recall of WordVec_Based_Model_2': 0.10476190476190476},\n",
      " 'Setup': [{'Name': 'WordVec_Based_Model_2'},\n",
      "           {'Similarity Measure and Minimum Threshold': ('cosine', 0.75)},\n",
      "           {'Top Value': 3},\n",
      "           {'Tokenizer': <utils.tokenizers.WordNetBased_LemmaTokenizer object at 0x7fb405b6bf60>}]}\n",
      "------------------------------------------------------------------\n",
      "{'Measures': {'Mean FScore of WordVec_Based_Model_6': 0.16666666666666663,\n",
      "              'Mean Precision of WordVec_Based_Model_6': 0.2,\n",
      "              'Mean Recall of WordVec_Based_Model_6': 0.14285714285714285},\n",
      " 'Setup': [{'Name': 'WordVec_Based_Model_6'},\n",
      "           {'Similarity Measure and Minimum Threshold': ('cosine', 0.75)},\n",
      "           {'Top Value': 5},\n",
      "           {'Tokenizer': <utils.tokenizers.WordNetBased_LemmaTokenizer object at 0x7fb42e848ac8>}]}\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "aux_functions.print_report_top_3_and_5_v3(results_df, metric=SimilarityMeasure.COSINE.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
